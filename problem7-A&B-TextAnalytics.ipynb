{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c755288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9cf1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74ce8dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c15a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7a11ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text= sent_tokenize(text)\n",
    "print(tokenized_text)\n",
    "#Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4f64427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words of English\n",
      "{'off', 'where', \"weren't\", 'all', 'y', \"hadn't\", 'both', 'through', 'her', 'yourselves', 'them', 'his', 'in', \"hasn't\", 'it', 'until', 'aren', 'against', 'not', \"isn't\", 's', 'theirs', 'as', 'you', \"you're\", 'their', 'no', 'about', 'will', 'from', 'on', 'ain', 'needn', 'those', 'she', 'up', \"won't\", 'do', 'only', 'this', 'any', 'yourself', 'your', 'have', 'by', 'just', 'if', 'over', 'herself', 'for', 'didn', 'after', 'again', 'the', 'out', 'haven', 'weren', 'what', 'my', 'mightn', 'an', 'these', 'had', 'each', \"couldn't\", 'ma', 'nor', \"should've\", 'so', 'him', 'below', \"shouldn't\", 'down', 'that', 'ourselves', 'm', 'are', 'is', \"shan't\", \"needn't\", 'but', 't', 'our', 'been', 'mustn', 'himself', 'being', 'doesn', 'whom', \"you'll\", 'some', 'me', 'while', 'wouldn', 'then', 'has', 'more', 'shan', 'with', \"she's\", 'very', 'than', 'at', 'own', \"wouldn't\", 'myself', 'wasn', 'were', 'further', 'to', 'we', 'few', 'a', 'd', 'couldn', 'don', 'why', \"it's\", 'once', 'they', 'of', 'he', 'during', 'i', 'can', 'should', 'doing', \"wasn't\", \"you've\", 'too', 'o', 'ours', 're', 'and', 'does', 'yours', 'into', 'same', 'themselves', \"mustn't\", 'or', 'here', 'having', 'other', 'most', 'll', \"aren't\", 'itself', 'was', 'there', 'above', \"don't\", 'such', 'shouldn', 'who', \"haven't\", 'between', 'before', 'now', 'isn', 'its', 'because', \"you'd\", 've', 'when', \"didn't\", 'how', 'under', 'hers', 'won', 'hasn', \"that'll\", 'did', \"doesn't\", 'be', 'hadn', \"mightn't\", 'am', 'which'}\n"
     ]
    }
   ],
   "source": [
    "print(\"stop words of English\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b09e801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "word_tokens = word_tokenize(example_sent)\n",
    " \n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    " \n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    " \n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c289e15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words: \n",
    "    rootWord=ps.stem(w) \n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8dc3033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Yash\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization: \n",
    "    print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69c7d124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('pink', 'NN')]\n",
      "[('sweater', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('her', 'PRP$')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The pink sweater fit her perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words: \n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a86f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8da86572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7de6ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6615d11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sun', 'Planet', 'Mars', 'from', 'largest', 'planet', 'fourth', 'is', 'the', 'Jupiter'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Jupiter is the largest Planet\"\n",
    "second_sentence = \"Mars is the fourth planet from the Sun\"\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1922c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed23b0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sun</th>\n",
       "      <th>Planet</th>\n",
       "      <th>Mars</th>\n",
       "      <th>from</th>\n",
       "      <th>largest</th>\n",
       "      <th>planet</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>Jupiter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sun  Planet  Mars  from  largest  planet  fourth  is  the  Jupiter\n",
       "0    0       1     0     0        1       0       0   1    1        1\n",
       "1    1       0     1     1        0       1       1   1    2        0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff9f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cfbd1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sun</th>\n",
       "      <th>Planet</th>\n",
       "      <th>Mars</th>\n",
       "      <th>from</th>\n",
       "      <th>largest</th>\n",
       "      <th>planet</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>the</th>\n",
       "      <th>Jupiter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sun  Planet   Mars   from  largest  planet  fourth     is   the  Jupiter\n",
       "0  0.000     0.2  0.000  0.000      0.2   0.000   0.000  0.200  0.20      0.2\n",
       "1  0.125     0.0  0.125  0.125      0.0   0.125   0.125  0.125  0.25      0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b941d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c47c45c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sun': 0.3010299956639812,\n",
       " 'Planet': 0.3010299956639812,\n",
       " 'Mars': 0.3010299956639812,\n",
       " 'from': 0.3010299956639812,\n",
       " 'largest': 0.3010299956639812,\n",
       " 'planet': 0.3010299956639812,\n",
       " 'fourth': 0.3010299956639812,\n",
       " 'is': 0.3010299956639812,\n",
       " 'the': 0.3010299956639812,\n",
       " 'Jupiter': 0.3010299956639812}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97fd0777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sun    Planet      Mars      from   largest    planet    fourth  \\\n",
      "0  0.000000  0.060206  0.000000  0.000000  0.060206  0.000000  0.000000   \n",
      "1  0.037629  0.000000  0.037629  0.037629  0.000000  0.037629  0.037629   \n",
      "\n",
      "         is       the   Jupiter  \n",
      "0  0.060206  0.060206  0.060206  \n",
      "1  0.037629  0.075257  0.000000  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee3a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
